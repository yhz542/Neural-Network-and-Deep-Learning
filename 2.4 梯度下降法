成本函数J(w,b)表示误差函数L的平均值，那么L的值越小，误差越低，算法的性能也就越好.

所以J(w,b)越小越好

为了使J(w,b)最小，则相当于在函数J的曲面（3维 两个参数）或曲线（2维 单个参数）上找到值最低的那一点，当J是一个凹函数时，（构成凹曲面或者凹曲线），为了得到更好的参数，无论参数在哪一点，最后都应该收敛于同一个点（或者一个很小的范围，即接近于最优值）。

梯度下降法（Gradient Descent）
从初始点开始朝最陡峭（斜率最大）的方向走一步，在走了一步后，再次检测哪个方向最为陡峭，如果没有陡峭的方向就停下来（最优点），否则选出最陡的方向并朝该方向继续前行，接下去不断迭代，直到停下来为止。

选出最陡的方向因为斜率最大，梯度下降速度最快（收敛速度最快）

举例 J(w)=w^2 （为了便于理解，省去b，将3维降为2维）

w的更新算法为:

	  d(J(w))
w = w - a————————
	   d w

当 w 到达最低点前，重复执行上式。

a表示学习率，用于控制每一次迭代或者梯度下降法中的步长（如何选择学习率a在后序部分讨论）

函数 J 对 w 求导，在平面坐标轴中的结果为函数 J 在 w 这一点的斜率。

举例

对于 J(w)=w^2 而言 

d(J)/d w = 2 w

当w>0时,2 w >0 ,a(2 w )>0, w(new) = w(old)-a(2w);即w会不断的减小。

当w<0时 2 w <0 ,a(2 w )<0, w(new) = w(old)-a(2w),因为a(2w)<0，所以w(new)=w(old)+a(2|w|);即w会不断增大。 

无论w>0还是w<0,当w=0时 2w=0此时 a(2 w )=0 所以w(new)=w(old)即 w的值不会变动。（换句话说当w=0时，斜率为0）此时 w 取得全局最优值 J(w)获得最小值

显然 w^2 为一个凹函数，当w=0时，函数取到最小值。J(w)=0

拓展到J(w,b)

	  d(J(w,b))
w = w - a—————————
	     d w


d(J(w,b))/d w 可理解为J函数在w方向的斜率


	  d(J(w,b))
b = b - a—————————
	     d b

同理 d(J(w,b))/d b 可理解为J函数在b方向的斜率

以上两式为实际更新中的操作
