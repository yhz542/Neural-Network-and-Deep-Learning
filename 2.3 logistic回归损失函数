
		  1
sigmoid(z) = ——————————
	      1 + e^(-z)


 y^ = sigmoid ( wTx + b )


通过训练集{(x^(1),y^(1)),（x^(2),y^(2)）......,（x^(m),y^(m))}训练，我们试图y^(i)约等于y(i)

即第 i 个样本的 y^(y=1的概率) 约等于其实际结果 y

y^(i)=sigmoid( wT x(i) + b )

Loss(error) function(损失函数,误差函数（用于衡量算法的运行情况）一般可以定义为):

		    2
	  （ y^- y ）		   2
L(y^,y) = —————————  or ( y^ - y )
	      2

在logistic回归中，不会用这个误差函数，因为这个函数并不是凸的，可能是类正弦波，不断起伏震荡的函数，所以会有很多个局部最优解，梯度下降法，可能找不到全局最优值

logistic回归中的Loss function:


L(y^,y) = -[ y log (y^)  + ( 1 - y )log( 1 - y^)] （底数默认>1）

误差函数的值越小越好

if y = 1:

L(y^,1) = -log(y^)  

要使L的值小,那么log(y^)值就要大,y^的值也就越大;因为sigmoid的原因，y^最大只能为1。

所以当y=1时，L越小,y^就越接近1，也就越接近y.

if y = 0:

L(y^,0) = -log(1-y^)

L越小,(1-y^)越大，y^越小，y^最小只能为0。

所以当y=0时,L越小,y^越接近0，也就越接近y。

Loss function是在单个训练样本中定义的，它衡量了在单个训练样本上的表现

cost function(成本函数)：衡量在全体训练样本上的表现（算法的整体性能）


J(w,b)=(1/m)( L(y^(1),y(1)) + L( y^(2), y(2) ) + .....+ L (y^(m),y(m)))

上式等于 -(1/m) sum [y(i) log (y^(i))  + ( 1 - y(i) )log( 1 - y^(i))] (i from 1 to m)

